{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e80412d-f88c-47e5-bfb7-3341537a1820",
   "metadata": {},
   "source": [
    "# Data Mining Week 8 with Professor Sloan\n",
    "\n",
    "## Maggie Boles\n",
    "\n",
    "### From Blackboard: Using the Kaggle NER corpus (ner_database.csv), which you can also find in our GitHub, create a NER tagger using Scikit-learn, which implies creating the NER model.\n",
    "    I highly encourage you to look at the Author's Notebook for Chapter 8. In the text, this all starts on p. 545 and note the Author's GitHub is a little different than what's in the text. Note that building this model is going to take some time so plan accordingly. For example, the fit() alone was 3 minutes (not too bad, but it could take much longer on your machine).\n",
    "    There's also a package installed by the author in his Notebook (sklearn-crfsuite). He installs it in-line in the Notebook, which may not work with Visual Studio Code. But you can just install it at a terminal.\n",
    "    Run the following sentence through your tagger: “Fourteen days ago, Emperor Palpatine left San Diego, CA for Tatooine to follow Luke Skywalker.” Report on the tags applied to the sentence.\n",
    "    Run the same sentence through spaCy’s NER engine.\n",
    "    Compare and contrast the results – you can do this in your Jupyter Notebook or as a comment in your .py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f930dc4-518d-4a33-83cf-6223979eb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggi\\AppData\\Local\\Temp\\ipykernel_5776\\1945087056.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\")          # VERY IMPORTANT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1  Sentence: 1             of   IN   O\n",
      "2  Sentence: 1  demonstrators  NNS   O\n",
      "3  Sentence: 1           have  VBP   O\n",
      "4  Sentence: 1        marched  VBN   O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggi\\AppData\\Local\\Temp\\ipykernel_5776\\1945087056.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda s: [(w, t) for w, t in zip(s[\"Word\"], s[\"Tag\"])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47959 sentences loaded\n",
      "Training CRF…\n",
      "Done!\n",
      "\n",
      "=== CUSTOM CRF TAGGER ===\n",
      "Fourteen        O\n",
      "days            O\n",
      "ago             O\n",
      ",               O\n",
      "Emperor         B-per\n",
      "Palpatine       I-per\n",
      "left            O\n",
      "San             B-geo\n",
      "Diego           I-geo\n",
      ",               O\n",
      "CA              B-org\n",
      "for             I-org\n",
      "Tatooine        I-org\n",
      "to              O\n",
      "follow          O\n",
      "Luke            B-org\n",
      "Skywalker       I-org\n",
      ".               O\n",
      "\n",
      "=== spaCy NER ===\n",
      "Fourteen days ago    DATE\n",
      "Palpatine            PERSON\n",
      "San Diego            GPE\n",
      "CA for Tatooine      WORK_OF_ART\n",
      "Luke Skywalker       PERSON\n"
     ]
    }
   ],
   "source": [
    "#PATCH FOR BOTTLENECK ISSUE I WAS HAVING\n",
    "import os\n",
    "os.environ[\"PANDAS_NO_BOTTLENECK\"] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*bottleneck.*\")\n",
    "\n",
    "# (Resource use: Sarkar, D. (2019). Text analytics with python: A practitioner’s Guide to Natural Language Processing. Apress. (Chapter 8))\n",
    "# 2. IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF\n",
    "import spacy\n",
    "\n",
    "#LOAD DATA\n",
    "df = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "df = df.fillna(method=\"ffill\")\n",
    "print(df.head())\n",
    "\n",
    "#GROUP INTO SENTENCES\n",
    "sentences = (\n",
    "    df.groupby(\"Sentence #\")\n",
    "      .apply(lambda s: [(w, t) for w, t in zip(s[\"Word\"], s[\"Tag\"])])\n",
    "      .tolist()\n",
    ")\n",
    "print(f\"{len(sentences)} sentences loaded\")\n",
    "\n",
    "#FEATURE FUNCTION \n",
    "def word2features(sent, i):\n",
    "    word = str(sent[i][0])\n",
    "    features = {\n",
    "        \"bias\": 1.0,\n",
    "        \"word.lower()\": word.lower(),\n",
    "        \"word[-3:]\": word[-3:],\n",
    "        \"word[-2:]\": word[-2:],\n",
    "        \"word.isupper()\": word.isupper(),\n",
    "        \"word.istitle()\": word.istitle(),\n",
    "        \"word.isdigit()\": word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        prev = str(sent[i-1][0])\n",
    "        features.update({\n",
    "            \"-1:word.lower()\": prev.lower(),\n",
    "            \"-1:word.istitle()\": prev.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "    if i < len(sent)-1:\n",
    "        nxt = str(sent[i+1][0])\n",
    "        features.update({\n",
    "            \"+1:word.lower()\": nxt.lower(),\n",
    "            \"+1:word.istitle()\": nxt.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "    return features\n",
    "\n",
    "def sent2features(s): return [word2features(s, i) for i in range(len(s))]\n",
    "def sent2labels(s):   return [label for _, label in s]\n",
    "\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s)   for s in sentences]\n",
    "\n",
    "#TRAIN / TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#TRAIN CRF\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1,\n",
    "          max_iterations=50, all_possible_transitions=False)\n",
    "print(\"Training CRF…\")\n",
    "crf.fit(X_train, y_train)\n",
    "print(\"Done!\")\n",
    "\n",
    "#TEST SENTENCE – CUSTOM TAGGER\n",
    "test_sent = \"Fourteen days ago , Emperor Palpatine left San Diego , CA for Tatooine to follow Luke Skywalker .\".split()\n",
    "dummy = [(w, \"O\") for w in test_sent]          # dummy tags\n",
    "feat  = sent2features(dummy)\n",
    "pred  = crf.predict_single(feat)\n",
    "\n",
    "print(\"\\n=== CUSTOM CRF TAGGER ===\")\n",
    "for w, t in zip(test_sent, pred):\n",
    "    print(f\"{w:15} {t}\")\n",
    "\n",
    "\n",
    "#spaCy NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join(test_sent))\n",
    "print(\"\\n=== spaCy NER ===\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11daa33b-ec07-472f-8333-64327f052b56",
   "metadata": {},
   "source": [
    "##### From my output of this model when I just objectively look at the custom tagger and the NER I see that the custom tagger categorized each word in the sentence, and it is nice to see how the CRF categorizes each word and it did chunk Emporer Palpatine, San Diego, & Luke Skywalker giving us the Inside and beginning tagging we see, albeit Luke is a person not an organization (but it did group the two together), the CA for Tatooine categorizing as an Organization could be an error in the training for the CRF, the spaCY NER categorized this one as a Geopolitical entity which is pretty funny. I think this again could be from the structure and the grouping that is occuring, it makes me think of something like: \"Californinians for Tatooine Politics\" and I think that is a little silly and fun. For the spaCY NER we get a different output where it correctly grouped a date (fourteen days ago), it missed grouping Emporer with Palpatine, but still correctly identified Palpatine as a person. San Diego is a GPE (also fails to group with CA), CA for Tatooine as a WOA which does correctly label this (though still not accurate) by identifying that Tattoine is part of classifying titles within text, and Luke Skywalker as a Person. \n",
    "\n",
    "##### I think overall we can do some comparing and contrasting. With the CRF we do have to train the model, and then we can put our sentence(s) through it with that though it did take a little bit of time (not too bad for my rig though as I recently upgraded it over the summer because I was running a 2070 with an i5 processor, so my computer didn't struggle too bad with this part), the tag scheme is the BIO that we previously had covered, we used tokenization to split on spaces, we have entity grouping, and we have a lot more control over features and training data so we could potentially continue to fine tune this model if we wanted to. \n",
    "\n",
    "##### For the spaCY NER this is not pre-trained, has a flat tag scheme, the tokenization handles punctuation and contractions, entity grouping is apparent, there may be some tuning we could do, but I'm not 100% and from some of the things I read it can be fine-tuned but it can be complex. \n",
    "\n",
    "##### The one thing I think that is important to note agian is that both of these methods missclassify CA for Tatooine and the CRF classified Luke as an ORG. While neither was a perfect model, they seemed to both struggle on one part of this at the same time, it could be strategically confusing to essentially flag this error on purpose, or the real and fake locations cause this problem. Or specifically with the training model, this could possibly be a time where ensemble training could come into play and help our model tuning and increasing accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
